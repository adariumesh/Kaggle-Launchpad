export interface NotebookOptions {
  includeEDA: boolean;
  includeBaseline: boolean;
  competitionType: 'classification' | 'regression' | 'nlp' | 'computer-vision' | 'other';
  selectedModel: string;
  missingValueStrategy: 'median' | 'mean' | 'mode' | 'advanced';
  includeAdvancedFeatureEngineering: boolean;
  crossValidationFolds: number;
  hyperparameterTuning: boolean;
}

export interface KaggleNotebook {
  path: string;
  content: string;
  expectedScore?: string;
  description: string;
}

export class NotebookGenerator {
  private static detectCompetitionType(name: string): NotebookOptions['competitionType'] {
    const lowerName = name.toLowerCase();
    
    if (lowerName.includes('nlp') || lowerName.includes('sentiment') || lowerName.includes('text')) {
      return 'nlp';
    }
    if (lowerName.includes('image') || lowerName.includes('vision') || lowerName.includes('digit')) {
      return 'computer-vision';
    }
    if (lowerName.includes('price') || lowerName.includes('sales') || lowerName.includes('revenue')) {
      return 'regression';
    }
    return 'classification';
  }

  private static getAvailableModels(competitionType: NotebookOptions['competitionType']): string[] {
    const modelMap = {
      'classification': ['XGBoost', 'RandomForest', 'LogisticRegression', 'LightGBM', 'CatBoost', 'SVM'],
      'regression': ['XGBoost', 'RandomForest', 'LinearRegression', 'LightGBM', 'CatBoost', 'SVR'],
      'nlp': ['LogisticRegression', 'RandomForest', 'XGBoost', 'NaiveBayes', 'SVM'],
      'computer-vision': ['RandomForest', 'XGBoost', 'CNN', 'ResNet', 'EfficientNet'],
      'other': ['XGBoost', 'RandomForest', 'LogisticRegression', 'LightGBM']
    };
    return modelMap[competitionType];
  }

  static async generateKaggleNotebook(
    competition: string,
    options: Omit<NotebookOptions, 'competitionType'>
  ): Promise<KaggleNotebook> {
    const competitionName = this.extractCompetitionName(competition);
    const competitionType = this.detectCompetitionType(competitionName);
    const fullOptions = { ...options, competitionType };

    // Validate model selection
    const availableModels = this.getAvailableModels(competitionType);
    if (!availableModels.includes(fullOptions.selectedModel)) {
      fullOptions.selectedModel = availableModels[0]; // Default to first available
    }

    const notebook = this.createSingleNotebook(competitionName, fullOptions);
    
    return {
      path: `${competitionName}-kaggle-notebook.ipynb`,
      content: notebook,
      expectedScore: this.getExpectedScore(competitionName, competitionType, fullOptions.selectedModel),
      description: `Complete Kaggle notebook for ${this.formatTitle(competitionName)} competition using ${fullOptions.selectedModel}`
    };
  }

  private static extractCompetitionName(input: string): string {
    const urlMatch = input.match(/kaggle\.com\/competitions\/([^\/\?]+)/);
    return urlMatch ? urlMatch[1] : input.trim();
  }

  private static formatTitle(name: string): string {
    return name
      .split('-')
      .map(word => word.charAt(0).toUpperCase() + word.slice(1))
      .join(' ');
  }

  private static getExpectedScore(competition: string, type: NotebookOptions['competitionType'], model: string): string {
    const baseScores: Record<string, string> = {
      'titanic': '0.77-0.82',
      'house-prices-advanced-regression-techniques': '0.12-0.15 RMSE',
      'digit-recognizer': '0.95-0.98',
      'nlp-getting-started': '0.80-0.85',
    };
    
    const modelBonus = ['XGBoost', 'LightGBM', 'CatBoost'].includes(model) ? ' (optimized)' : '';
    const baseScore = baseScores[competition] || (type === 'regression' ? '0.10-0.20 RMSE' : '0.75-0.85');
    
    return baseScore + modelBonus;
  }

  private static createSingleNotebook(competition: string, options: NotebookOptions): string {
    const title = this.formatTitle(competition);
    const isRegression = options.competitionType === 'regression';
    const isNLP = options.competitionType === 'nlp';
    const isCV = options.competitionType === 'computer-vision';

    const notebookContent = {
      cells: [
        // Header Cell
        {
          cell_type: "markdown",
          metadata: {},
          source: [
            `# üöÄ ${title} - Advanced Kaggle Solution\n`,
            `**Generated by Kaggle Launchpad** | Model: **${options.selectedModel}** | Expected Score: ${this.getExpectedScore(competition, options.competitionType, options.selectedModel)}\n\n`,
            `## üìã Competition Overview\n`,
            `- **Type**: ${options.competitionType.charAt(0).toUpperCase() + options.competitionType.slice(1)}\n`,
            `- **Model**: ${options.selectedModel}\n`,
            `- **Goal**: ${this.getCompetitionGoal(options.competitionType)}\n`,
            `- **Evaluation**: ${this.getEvaluationMetric(options.competitionType)}\n`,
            `- **Cross-Validation**: ${options.crossValidationFolds}-fold\n`,
            `- **Hyperparameter Tuning**: ${options.hyperparameterTuning ? 'Enabled' : 'Disabled'}\n\n`,
            `## üéØ Notebook Structure\n`,
            `1. **Setup & Data Loading** - Import libraries and load Kaggle data\n`,
            `2. **Exploratory Data Analysis** - Understand the data patterns\n`,
            `3. **Advanced Data Preprocessing** - Clean and engineer features\n`,
            `4. **Model Training & Validation** - Build and optimize model\n`,
            `5. **Predictions & Submission** - Generate final submission\n\n`,
            `---`
          ]
        },

        // Setup Cell
        {
          cell_type: "code",
          execution_count: null,
          metadata: {},
          outputs: [],
          source: [
            "# ====== KAGGLE ENVIRONMENT SETUP ======\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore')\n\n",
            "# Kaggle-specific imports\n",
            "import os\n",
            "print(f\"Kaggle environment: {os.path.exists('/kaggle/input')}\")\n",
            "print(f\"GPU available: {os.path.exists('/opt/conda/bin/nvidia-smi')}\")\n\n",
            "# ML Libraries\n",
            this.getMLImports(options),
            "\n# Visualization setup\n",
            "plt.style.use('seaborn-v0_8')\n",
            "sns.set_palette('husl')\n",
            "pd.set_option('display.max_columns', None)\n\n",
            "print(\"‚úÖ Setup complete!\")\n",
            "print(f\"ü§ñ Selected Model: {options.selectedModel}\")\n",
            "print(f\"üîß Missing Value Strategy: {options.missingValueStrategy}\")\n",
            "print(f\"‚öôÔ∏è Advanced Feature Engineering: {options.includeAdvancedFeatureEngineering}\")"
          ]
        },

        // Data Loading Cell
        {
          cell_type: "code",
          execution_count: null,
          metadata: {},
          outputs: [],
          source: [
            "# ====== DATA LOADING ======\n",
            "# Kaggle input paths\n",
            `INPUT_DIR = '/kaggle/input/${competition}'\n`,
            "TRAIN_PATH = f'{INPUT_DIR}/train.csv'\n",
            "TEST_PATH = f'{INPUT_DIR}/test.csv'\n",
            "SAMPLE_SUB_PATH = f'{INPUT_DIR}/sample_submission.csv'\n\n",
            "# Load datasets\n",
            "print(\"üìÇ Loading data...\")\n",
            "train_df = pd.read_csv(TRAIN_PATH)\n",
            "test_df = pd.read_csv(TEST_PATH)\n",
            "sample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n\n",
            "print(f\"Train shape: {train_df.shape}\")\n",
            "print(f\"Test shape: {test_df.shape}\")\n",
            "print(f\"Sample submission shape: {sample_submission.shape}\")\n\n",
            "# Display first few rows\n",
            "print(\"\\nüìä First 5 rows of training data:\")\n",
            "train_df.head()"
          ]
        },

        // EDA Cell (if enabled)
        ...(options.includeEDA ? [{
          cell_type: "markdown",
          metadata: {},
          source: ["## üîç Advanced Exploratory Data Analysis"]
        }, {
          cell_type: "code",
          execution_count: null,
          metadata: {},
          outputs: [],
          source: [
            "# ====== ADVANCED EDA FUNCTIONS ======\n",
            "def comprehensive_eda(df, name='Dataset'):\n",
            "    print(f\"\\n=== {name.upper()} COMPREHENSIVE ANALYSIS ===\")\n",
            "    print(f\"Shape: {df.shape}\")\n",
            "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
            "    \n",
            "    # Data types analysis\n",
            "    print(f\"\\nData types distribution:\")\n",
            "    dtype_counts = df.dtypes.value_counts()\n",
            "    for dtype, count in dtype_counts.items():\n",
            "        print(f\"  {dtype}: {count} columns\")\n",
            "    \n",
            "    # Missing values analysis\n",
            "    missing_data = df.isnull().sum().sort_values(ascending=False)\n",
            "    missing_percent = (missing_data / len(df)) * 100\n",
            "    missing_df = pd.DataFrame({\n",
            "        'Missing Count': missing_data,\n",
            "        'Missing Percentage': missing_percent\n",
            "    })\n",
            "    print(f\"\\nMissing values summary:\")\n",
            "    print(missing_df[missing_df['Missing Count'] > 0].head(10))\n",
            "    \n",
            "    # Cardinality analysis\n",
            "    print(f\"\\nCardinality analysis (categorical columns):\")\n",
            "    cat_cols = df.select_dtypes(include=['object']).columns\n",
            "    for col in cat_cols[:10]:  # Top 10 categorical columns\n",
            "        unique_count = df[col].nunique()\n",
            "        print(f\"  {col}: {unique_count} unique values\")\n",
            "    \n",
            "    return df.describe(include='all')\n\n",
            "def advanced_target_analysis(df, target_col):\n",
            "    if target_col not in df.columns:\n",
            "        print(f\"Target column '{target_col}' not found\")\n",
            "        return\n",
            "    \n",
            "    plt.figure(figsize=(15, 10))\n",
            "    \n",
            "    # Distribution plot\n",
            "    plt.subplot(2, 3, 1)\n",
            "    if df[target_col].dtype in ['int64', 'float64']:\n",
            "        df[target_col].hist(bins=50, edgecolor='black', alpha=0.7)\n",
            "        plt.title(f'{target_col} Distribution')\n",
            "    else:\n",
            "        df[target_col].value_counts().plot(kind='bar')\n",
            "        plt.title(f'{target_col} Value Counts')\n",
            "        plt.xticks(rotation=45)\n",
            "    \n",
            "    # Box plot\n",
            "    plt.subplot(2, 3, 2)\n",
            "    if df[target_col].dtype in ['int64', 'float64']:\n",
            "        df[target_col].plot(kind='box')\n",
            "        plt.title(f'{target_col} Box Plot')\n",
            "    \n",
            "    # QQ plot for normality check\n",
            "    if df[target_col].dtype in ['int64', 'float64']:\n",
            "        from scipy import stats\n",
            "        plt.subplot(2, 3, 3)\n",
            "        stats.probplot(df[target_col].dropna(), dist=\"norm\", plot=plt)\n",
            "        plt.title(f'{target_col} Q-Q Plot')\n",
            "    \n",
            "    # Skewness and kurtosis\n",
            "    if df[target_col].dtype in ['int64', 'float64']:\n",
            "        skewness = df[target_col].skew()\n",
            "        kurtosis = df[target_col].kurtosis()\n",
            "        print(f\"\\nüìä {target_col} Statistics:\")\n",
            "        print(f\"  Skewness: {skewness:.4f}\")\n",
            "        print(f\"  Kurtosis: {kurtosis:.4f}\")\n",
            "        print(f\"  Outliers (IQR method): {len(df[df[target_col] < df[target_col].quantile(0.25) - 1.5 * (df[target_col].quantile(0.75) - df[target_col].quantile(0.25))])} lower, {len(df[df[target_col] > df[target_col].quantile(0.75) + 1.5 * (df[target_col].quantile(0.75) - df[target_col].quantile(0.25))])} upper\")\n",
            "    \n",
            "    plt.tight_layout()\n",
            "    plt.show()\n\n",
            "def feature_correlation_analysis(df, target_col=None, top_n=15):\n",
            "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
            "    if len(numeric_cols) <= 1:\n",
            "        print(\"Not enough numeric columns for correlation analysis\")\n",
            "        return None\n",
            "    \n",
            "    correlation_matrix = df[numeric_cols].corr()\n",
            "    \n",
            "    # Heatmap\n",
            "    plt.figure(figsize=(12, 10))\n",
            "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
            "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
            "                center=0, fmt='.2f', square=True, cbar_kws={'shrink': 0.8})\n",
            "    plt.title('Feature Correlation Matrix')\n",
            "    plt.tight_layout()\n",
            "    plt.show()\n",
            "    \n",
            "    if target_col and target_col in correlation_matrix.columns:\n",
            "        target_corr = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
            "        print(f\"\\nüîó Top {top_n} features correlated with {target_col}:\")\n",
            "        print(target_corr.head(top_n + 1)[1:])  # Exclude self-correlation\n",
            "        \n",
            "        # Plot top correlations\n",
            "        plt.figure(figsize=(10, 6))\n",
            "        top_corr = target_corr.head(top_n + 1)[1:]\n",
            "        colors = ['red' if x < 0 else 'blue' for x in correlation_matrix[target_col][top_corr.index]]\n",
            "        plt.barh(range(len(top_corr)), correlation_matrix[target_col][top_corr.index], color=colors, alpha=0.7)\n",
            "        plt.yticks(range(len(top_corr)), top_corr.index)\n",
            "        plt.xlabel('Correlation with Target')\n",
            "        plt.title(f'Top {top_n} Feature Correlations with {target_col}')\n",
            "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
            "        plt.tight_layout()\n",
            "        plt.show()\n",
            "    \n",
            "    return correlation_matrix"
          ]
        }, {
          cell_type: "code",
          execution_count: null,
          metadata: {},
          outputs: [],
          source: [
            "# ====== COMPREHENSIVE DATA EXPLORATION ======\n",
            "# Basic info\n",
            "train_stats = comprehensive_eda(train_df, 'Training Data')\n",
            "test_stats = comprehensive_eda(test_df, 'Test Data')\n\n",
            "# Identify target column\n",
            this.getTargetIdentification(options.competitionType),
            "\n# Advanced target analysis\n",
            "if target_col:\n",
            "    print(f\"\\nüéØ Target column: {target_col}\")\n",
            "    print(f\"Target statistics:\\n{train_df[target_col].describe()}\")\n",
            "    advanced_target_analysis(train_df, target_col)\n\n",
            "# Comprehensive correlation analysis\n",
            "correlation_matrix = feature_correlation_analysis(train_df, target_col, top_n=15)\n\n",
            "# Feature importance preview (if we have numeric features)\n",
            "numeric_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
            "if target_col in numeric_features:\n",
            "    numeric_features.remove(target_col)\n",
            "\n",
            "if len(numeric_features) > 0 and target_col:\n",
            "    print(f\"\\nüìà Quick feature importance using mutual information:\")\n",
            "    from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
            "    from sklearn.preprocessing import LabelEncoder\n",
            "    \n",
            "    # Prepare data for mutual information\n",
            "    X_sample = train_df[numeric_features].fillna(train_df[numeric_features].median())\n",
            "    y_sample = train_df[target_col]\n",
            "    \n",
            "    if train_df[target_col].dtype == 'object':\n",
            "        le = LabelEncoder()\n",
            "        y_sample = le.fit_transform(y_sample.astype(str))\n",
            "        mi_scores = mutual_info_classif(X_sample, y_sample, random_state=42)\n",
            "    else:\n",
            "        mi_scores = mutual_info_regression(X_sample, y_sample, random_state=42)\n",
            "    \n",
            "    mi_df = pd.DataFrame({\n",
            "        'feature': numeric_features,\n",
            "        'mutual_info_score': mi_scores\n",
            "    }).sort_values('mutual_info_score', ascending=False)\n",
            "    \n",
            "    print(mi_df.head(10))\n",
            "    \n",
            "    # Plot mutual information scores\n",
            "    plt.figure(figsize=(10, 6))\n",
            "    top_mi = mi_df.head(15)\n",
            "    plt.barh(range(len(top_mi)), top_mi['mutual_info_score'], alpha=0.7)\n",
            "    plt.yticks(range(len(top_mi)), top_mi['feature'])\n",
            "    plt.xlabel('Mutual Information Score')\n",
            "    plt.title('Top 15 Features by Mutual Information')\n",
            "    plt.tight_layout()\n",
            "    plt.show()"
          ]
        }] : []),

        // Advanced Preprocessing Cell
        {
          cell_type: "markdown",
          metadata: {},
          source: ["## üõ†Ô∏è Advanced Data Preprocessing"]
        },
        {
          cell_type: "code",
          execution_count: null,
          metadata: {},
          outputs: [],
          source: [
            "# ====== ADVANCED PREPROCESSING PIPELINE ======\n",
            "def advanced_preprocess_data(train_df, test_df, target_col=None, strategy='", options.missingValueStrategy, "'):\n",
            "    \"\"\"Advanced preprocessing pipeline optimized for Kaggle with customizable strategies\"\"\"\n",
            "    print(\"üîß Starting advanced preprocessing...\")\n",
            "    print(f\"üìã Missing value strategy: {strategy}\")\n",
            "    print(f\"‚öôÔ∏è Advanced feature engineering: {options.includeAdvancedFeatureEngineering}\")\n",
            "    \n",
            "    # Combine for consistent preprocessing\n",
            "    train_len = len(train_df)\n",
            "    if target_col:\n",
            "        y = train_df[target_col].copy()\n",
            "        combined_df = pd.concat([train_df.drop(columns=[target_col]), test_df], ignore_index=True)\n",
            "    else:\n",
            "        y = None\n",
            "        combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
            "    \n",
            "    # Identify column types\n",
            "    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
            "    categorical_cols = combined_df.select_dtypes(include=['object']).columns.tolist()\n",
            "    datetime_cols = combined_df.select_dtypes(include=['datetime64']).columns.tolist()\n",
            "    \n",
            "    print(f\"üìä Column analysis:\")\n",
            "    print(f\"  Numeric: {len(numeric_cols)} columns\")\n",
            "    print(f\"  Categorical: {len(categorical_cols)} columns\")\n",
            "    print(f\"  Datetime: {len(datetime_cols)} columns\")\n",
            "    \n",
            this.getAdvancedMissingValueHandling(options.missingValueStrategy),
            "    \n",
            this.getAdvancedCategoricalEncoding(),
            "    \n",
            this.getDatetimeFeatureEngineering(),
            "    \n",
            this.getAdvancedFeatureEngineering(options),
            "    \n",
            "    # Feature scaling (if needed)\n",
            "    if '", options.selectedModel, "' in ['LogisticRegression', 'SVM', 'SVR', 'NeuralNetwork']:\n",
            "        print(\"üîÑ Applying feature scaling for model compatibility...\")\n",
            "        from sklearn.preprocessing import StandardScaler\n",
            "        scaler = StandardScaler()\n",
            "        \n",
            "        # Only scale numeric columns\n",
            "        numeric_cols_final = combined_df.select_dtypes(include=[np.number]).columns\n",
            "        combined_df[numeric_cols_final] = scaler.fit_transform(combined_df[numeric_cols_final])\n",
            "    \n",
            "    # Split back to train and test\n",
            "    X_train = combined_df[:train_len].copy()\n",
            "    X_test = combined_df[train_len:].copy()\n",
            "    \n",
            "    print(f\"‚úÖ Advanced preprocessing complete!\")\n",
            "    print(f\"Training features shape: {X_train.shape}\")\n",
            "    print(f\"Test features shape: {X_test.shape}\")\n",
            "    print(f\"Feature count change: {train_df.shape[1] - 1} ‚Üí {X_train.shape[1]} (+{X_train.shape[1] - (train_df.shape[1] - 1)})\")\n",
            "    \n",
            "    return X_train, X_test, y\n\n",
            "# Apply advanced preprocessing\n",
            "X_train, X_test, y = advanced_preprocess_data(train_df, test_df, target_col, '", options.missingValueStrategy, "')"
          ]
        },

        // Model Training Cell (if enabled)
        ...(options.includeBaseline ? [{
          cell_type: "markdown",
          metadata: {},
          source: ["## ü§ñ Advanced Model Training & Validation"]
        }, {
          cell_type: "code",
          execution_count: null,
          metadata: {},
          outputs: [],
          source: [
            "# ====== ADVANCED MODEL TRAINING ======\n",
            this.getAdvancedModelCode(options),
            "\n# Advanced train-validation split\n",
            "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
            "X_tr, X_val, y_tr, y_val = train_test_split(\n",
            "    X_train, y, test_size=0.2, random_state=42",
            isRegression ? "" : ", stratify=y",
            "\n)\n\n",
            "print(f\"Training set: {X_tr.shape}\")\n",
            "print(f\"Validation set: {X_val.shape}\")\n\n",
            options.hyperparameterTuning ? this.getHyperparameterTuning(options) : "",
            "\n# Train model\n",
            "print(\"üöÄ Training ", options.selectedModel, " model...\")\n",
            "model.fit(X_tr, y_tr)\n",
            "print(\"‚úÖ Training complete!\")\n\n",
            "# Cross-validation\n",
            "print(f\"\\nüîÑ Performing {options.crossValidationFolds}-fold cross-validation...\")\n",
            "from sklearn.model_selection import cross_val_score\n",
            isRegression ? 
              `cv_scores = cross_val_score(model, X_train, y, cv=${options.crossValidationFolds}, scoring='neg_mean_squared_error', n_jobs=-1)\ncv_rmse_scores = np.sqrt(-cv_scores)\nprint(f\"CV RMSE: {cv_rmse_scores.mean():.4f} (+/- {cv_rmse_scores.std() * 2:.4f})\")` :
              `cv_scores = cross_val_score(model, X_train, y, cv=${options.crossValidationFolds}, scoring='accuracy', n_jobs=-1)\nprint(f\"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")`,
            "\n\n# Validation predictions\n",
            "train_pred = model.predict(X_tr)\n",
            "val_pred = model.predict(X_val)\n\n",
            this.getAdvancedEvaluationCode(options),
            "\n# Feature importance analysis\n",
            this.getFeatureImportanceCode(options.selectedModel)
          ]
        }] : []),

        // Prediction & Submission Cell
        {
          cell_type: "markdown",
          metadata: {},
          source: ["## üì§ Final Predictions & Submission"]
        },
        {
          cell_type: "code",
          execution_count: null,
          metadata: {},
          outputs: [],
          source: [
            "# ====== GENERATE FINAL PREDICTIONS ======\n",
            "print(\"üîÆ Generating test predictions...\")\n",
            options.includeBaseline ? "test_predictions = model.predict(X_test)" : this.getSimplePredictions(options.competitionType),
            "\n\n# Create submission file\n",
            "submission = sample_submission.copy()\n",
            "submission.iloc[:, 1] = test_predictions  # Assuming second column is target\n\n",
            "# Save submission\n",
            "submission.to_csv('submission.csv', index=False)\n",
            "print(\"üíæ Submission saved as 'submission.csv'\")\n\n",
            "# Display submission info\n",
            "print(f\"\\nüìä Submission Statistics:\")\n",
            "print(f\"Shape: {submission.shape}\")\n",
            "print(f\"Predictions range: {test_predictions.min():.4f} to {test_predictions.max():.4f}\")\n",
            "print(f\"Predictions mean: {test_predictions.mean():.4f}\")\n",
            "print(f\"Predictions std: {test_predictions.std():.4f}\")\n",
            "\n# Prediction distribution\n",
            "plt.figure(figsize=(10, 4))\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.hist(test_predictions, bins=50, alpha=0.7, edgecolor='black')\n",
            "plt.title('Test Predictions Distribution')\n",
            "plt.xlabel('Predicted Values')\n",
            "plt.ylabel('Frequency')\n",
            "\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.boxplot(test_predictions)\n",
            "plt.title('Test Predictions Box Plot')\n",
            "plt.ylabel('Predicted Values')\n",
            "plt.tight_layout()\n",
            "plt.show()\n\n",
            "print(f\"\\nFirst 10 predictions:\")\n",
            "print(submission.head(10))\n\n",
            "# Final validation\n",
            "print(f\"\\n‚úÖ Advanced submission ready!\")\n",
            "print(f\"üéØ Model: {options.selectedModel}\")\n",
            "print(f\"üìà Expected score: {this.getExpectedScore(competition, options.competitionType, options.selectedModel)}\")\n",
            "print(f\"üîß Features used: {X_train.shape[1]}\")\n",
            "print(f\"üí° Next steps: Submit to Kaggle and iterate with ensemble methods!\")"
          ]
        },

        // Advanced Tips & Next Steps Cell
        {
          cell_type: "markdown",
          metadata: {},
          source: [
            "## üéØ Advanced Optimization & Next Steps\n\n",
            "### üöÄ Model-Specific Improvements:\n",
            this.getModelSpecificTips(options.selectedModel),
            "\n### üîß Advanced Techniques:\n",
            this.getAdvancedTips(options.competitionType),
            "\n### üèÜ Ensemble Methods:\n",
            "- **Voting Classifier/Regressor**: Combine multiple models\n",
            "- **Stacking**: Use meta-learner to combine predictions\n",
            "- **Blending**: Weighted average of different models\n",
            "- **Bagging**: Bootstrap aggregating for variance reduction\n\n",
            "### üìä Advanced Validation:\n",
            "- **Time Series CV**: For temporal data\n",
            "- **Group K-Fold**: For grouped data\n",
            "- **Adversarial Validation**: Check train/test similarity\n",
            "- **Pseudo-labeling**: Use confident test predictions\n\n",
            "### üìö Resources:\n",
            `- [Competition Discussion](https://www.kaggle.com/competitions/${competition}/discussion)\n`,
            `- [Competition Data](https://www.kaggle.com/competitions/${competition}/data)\n`,
            "- [Kaggle Learn](https://www.kaggle.com/learn)\n",
            "- [Feature Engineering Guide](https://www.kaggle.com/learn/feature-engineering)\n",
            "- [Model Interpretation](https://www.kaggle.com/learn/machine-learning-explainability)\n\n",
            "---\n",
            `**Generated by Kaggle Launchpad** üöÄ | Model: ${options.selectedModel} | Advanced Configuration Enabled`
          ]
        }
      ],
      metadata: {
        kernelspec: {
          display_name: "Python 3",
          language: "python",
          name: "python3"
        },
        language_info: {
          codemirror_mode: { name: "ipython", version: 3 },
          file_extension: ".py",
          mimetype: "text/x-python",
          name: "python",
          nbconvert_exporter: "python",
          pygments_lexer: "ipython3",
          version: "3.10.0"
        }
      },
      nbformat: 4,
      nbformat_minor: 4
    };

    return JSON.stringify(notebookContent, null, 2);
  }

  private static getCompetitionGoal(type: NotebookOptions['competitionType']): string {
    const goals = {
      'classification': 'Predict categorical outcomes',
      'regression': 'Predict continuous numerical values',
      'nlp': 'Process and analyze text data',
      'computer-vision': 'Analyze and classify images',
      'other': 'Solve the specific competition challenge'
    };
    return goals[type];
  }

  private static getEvaluationMetric(type: NotebookOptions['competitionType']): string {
    const metrics = {
      'classification': 'Accuracy / F1-Score / AUC',
      'regression': 'RMSE / MAE / R¬≤',
      'nlp': 'Accuracy / F1-Score',
      'computer-vision': 'Accuracy / mAP',
      'other': 'Competition-specific metric'
    };
    return metrics[type];
  }

  private static getMLImports(options: NotebookOptions): string {
    const baseImports = `from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif, mutual_info_regression
from sklearn.preprocessing import PolynomialFeatures
from scipy import stats`;

    const modelSpecificImports = {
      'XGBoost': `\nfrom xgboost import XGBClassifier, XGBRegressor`,
      'RandomForest': `\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor`,
      'LogisticRegression': `\nfrom sklearn.linear_model import LogisticRegression`,
      'LinearRegression': `\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet`,
      'LightGBM': `\nfrom lightgbm import LGBMClassifier, LGBMRegressor`,
      'CatBoost': `\nfrom catboost import CatBoostClassifier, CatBoostRegressor`,
      'SVM': `\nfrom sklearn.svm import SVC, SVR`,
      'NaiveBayes': `\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB`,
      'CNN': `\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms`,
      'ResNet': `\nimport torch\nimport torchvision.models as models`,
      'EfficientNet': `\nimport torch\nimport timm`
    };

    return baseImports + (modelSpecificImports[options.selectedModel] || '');
  }

  private static getTargetIdentification(type: NotebookOptions['competitionType']): string {
    return `target_col = None
possible_targets = ['target', 'label', 'y', 'outcome', 'Survived', 'SalePrice', 'price', 'class', 'category']
for col in possible_targets:
    if col in train_df.columns:
        target_col = col
        break

if not target_col:
    # Try to identify target by position (usually last column)
    target_col = train_df.columns[-1]
    print(f"‚ö†Ô∏è Auto-detected target column: {target_col}")
else:
    print(f"‚úÖ Found target column: {target_col}")`;
  }

  private static getAdvancedMissingValueHandling(strategy: string): string {
    const strategies = {
      'median': `
    # Handle missing values with median strategy
    print("üìù Handling missing values with median strategy...")
    for col in numeric_cols:
        if combined_df[col].isnull().sum() > 0:
            median_val = combined_df[col].median()
            combined_df[col].fillna(median_val, inplace=True)
            print(f"  {col}: filled {combined_df[col].isnull().sum()} missing values with median {median_val:.2f}")`,
      
      'mean': `
    # Handle missing values with mean strategy
    print("üìù Handling missing values with mean strategy...")
    for col in numeric_cols:
        if combined_df[col].isnull().sum() > 0:
            mean_val = combined_df[col].mean()
            combined_df[col].fillna(mean_val, inplace=True)
            print(f"  {col}: filled {combined_df[col].isnull().sum()} missing values with mean {mean_val:.2f}")`,
      
      'mode': `
    # Handle missing values with mode strategy
    print("üìù Handling missing values with mode strategy...")
    for col in numeric_cols + categorical_cols:
        if combined_df[col].isnull().sum() > 0:
            mode_val = combined_df[col].mode()[0] if len(combined_df[col].mode()) > 0 else 0
            combined_df[col].fillna(mode_val, inplace=True)
            print(f"  {col}: filled {combined_df[col].isnull().sum()} missing values with mode {mode_val}")`,
      
      'advanced': `
    # Advanced missing value handling
    print("üìù Handling missing values with advanced strategies...")
    from sklearn.impute import KNNImputer, IterativeImputer
    
    # For numeric columns with < 50% missing: KNN imputation
    # For numeric columns with >= 50% missing: median
    # For categorical: mode or create 'Missing' category
    
    for col in numeric_cols:
        missing_pct = combined_df[col].isnull().sum() / len(combined_df)
        if missing_pct > 0:
            if missing_pct < 0.5:
                # Use KNN imputation for moderate missing data
                knn_imputer = KNNImputer(n_neighbors=5)
                combined_df[[col]] = knn_imputer.fit_transform(combined_df[[col]])
                print(f"  {col}: KNN imputation ({missing_pct:.1%} missing)")
            else:
                # Use median for high missing data
                median_val = combined_df[col].median()
                combined_df[col].fillna(median_val, inplace=True)
                print(f"  {col}: median imputation ({missing_pct:.1%} missing)")
    
    # Advanced categorical handling
    for col in categorical_cols:
        missing_pct = combined_df[col].isnull().sum() / len(combined_df)
        if missing_pct > 0:
            if missing_pct < 0.1:
                # Use mode for low missing data
                mode_val = combined_df[col].mode()[0] if len(combined_df[col].mode()) > 0 else 'Unknown'
                combined_df[col].fillna(mode_val, inplace=True)
                print(f"  {col}: mode imputation ({missing_pct:.1%} missing)")
            else:
                # Create 'Missing' category for high missing data
                combined_df[col].fillna('Missing', inplace=True)
                print(f"  {col}: 'Missing' category ({missing_pct:.1%} missing)")`
    };

    return strategies[strategy] || strategies['median'];
  }

  private static getAdvancedCategoricalEncoding(): string {
    return `
    # Advanced categorical encoding
    print("üè∑Ô∏è Advanced categorical encoding...")
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    
    label_encoders = {}
    for col in categorical_cols:
        unique_count = combined_df[col].nunique()
        
        if unique_count <= 10:
            # One-hot encoding for low cardinality
            dummies = pd.get_dummies(combined_df[col], prefix=col, drop_first=True)
            combined_df = pd.concat([combined_df.drop(columns=[col]), dummies], axis=1)
            print(f"  {col}: one-hot encoded ({unique_count} categories)")
        else:
            # Label encoding for high cardinality
            le = LabelEncoder()
            combined_df[col] = le.fit_transform(combined_df[col].astype(str))
            label_encoders[col] = le
            print(f"  {col}: label encoded ({unique_count} categories)")`;
  }

  private static getDatetimeFeatureEngineering(): string {
    return `
    # Datetime feature engineering
    if len(datetime_cols) > 0:
        print("üìÖ Extracting datetime features...")
        for col in datetime_cols:
            combined_df[f'{col}_year'] = combined_df[col].dt.year
            combined_df[f'{col}_month'] = combined_df[col].dt.month
            combined_df[f'{col}_day'] = combined_df[col].dt.day
            combined_df[f'{col}_dayofweek'] = combined_df[col].dt.dayofweek
            combined_df[f'{col}_quarter'] = combined_df[col].dt.quarter
            combined_df[f'{col}_is_weekend'] = (combined_df[col].dt.dayofweek >= 5).astype(int)
            combined_df = combined_df.drop(columns=[col])
            print(f"  {col}: extracted 6 datetime features")`;
  }

  private static getAdvancedFeatureEngineering(options: NotebookOptions): string {
    if (!options.includeAdvancedFeatureEngineering) {
      return `
    # Basic feature engineering
    print("‚öôÔ∏è Creating basic interaction features...")
    numeric_features = combined_df.select_dtypes(include=[np.number]).columns[:5]
    if len(numeric_features) >= 2:
        combined_df[f'{numeric_features[0]}_x_{numeric_features[1]}'] = combined_df[numeric_features[0]] * combined_df[numeric_features[1]]
        print(f"  Created interaction: {numeric_features[0]} √ó {numeric_features[1]}")`;
    }

    return `
    # Advanced feature engineering
    print("‚öôÔ∏è Creating advanced features...")
    numeric_features = combined_df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_features) >= 2:
        # Interaction features (top 5 numeric features)
        top_features = numeric_features[:5]
        for i in range(len(top_features)):
            for j in range(i+1, len(top_features)):
                feat1, feat2 = top_features[i], top_features[j]
                # Multiplication
                combined_df[f'{feat1}_x_{feat2}'] = combined_df[feat1] * combined_df[feat2]
                # Division (avoid division by zero)
                combined_df[f'{feat1}_div_{feat2}'] = combined_df[feat1] / (combined_df[feat2] + 1e-8)
                # Addition
                combined_df[f'{feat1}_plus_{feat2}'] = combined_df[feat1] + combined_df[feat2]
        print(f"  Created interaction features for top {len(top_features)} numeric features")
    
    # Polynomial features (degree 2) for top 3 features
    if len(numeric_features) >= 3:
        from sklearn.preprocessing import PolynomialFeatures
        top_3_features = numeric_features[:3]
        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)
        poly_features = poly.fit_transform(combined_df[top_3_features])
        poly_feature_names = [f"poly_{name}" for name in poly.get_feature_names_out(top_3_features)]
        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=combined_df.index)
        # Only keep new polynomial features (not original ones)
        new_poly_features = [col for col in poly_df.columns if col not in top_3_features]
        combined_df = pd.concat([combined_df, poly_df[new_poly_features]], axis=1)
        print(f"  Created {len(new_poly_features)} polynomial features")
    
    # Statistical features
    if len(numeric_features) >= 3:
        # Row-wise statistics
        combined_df['row_mean'] = combined_df[numeric_features].mean(axis=1)
        combined_df['row_std'] = combined_df[numeric_features].std(axis=1)
        combined_df['row_min'] = combined_df[numeric_features].min(axis=1)
        combined_df['row_max'] = combined_df[numeric_features].max(axis=1)
        combined_df['row_median'] = combined_df[numeric_features].median(axis=1)
        print("  Created row-wise statistical features")
    
    # Log and square root transformations for skewed features
    for col in numeric_features[:10]:  # Top 10 numeric features
        if combined_df[col].min() >= 0:  # Only for non-negative values
            skewness = combined_df[col].skew()
            if abs(skewness) > 1:  # Highly skewed
                combined_df[f'{col}_log1p'] = np.log1p(combined_df[col])
                combined_df[f'{col}_sqrt'] = np.sqrt(combined_df[col])
                print(f"  Created log and sqrt transforms for {col} (skewness: {skewness:.2f})")
    
    # Binning for continuous variables
    for col in numeric_features[:5]:  # Top 5 numeric features
        combined_df[f'{col}_binned'] = pd.qcut(combined_df[col], q=5, labels=False, duplicates='drop')
        print(f"  Created binned version of {col}")`;
  }

  private static getAdvancedModelCode(options: NotebookOptions): string {
    const modelConfigs = {
      'XGBoost': {
        'classification': `# Initialize XGBoost Classifier with optimized parameters
model = XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='logloss',
    n_jobs=-1
)`,
        'regression': `# Initialize XGBoost Regressor with optimized parameters
model = XGBRegressor(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)`
      },
      'RandomForest': {
        'classification': `# Initialize Random Forest Classifier with optimized parameters
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)`,
        'regression': `# Initialize Random Forest Regressor with optimized parameters
model = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)`
      },
      'LogisticRegression': {
        'classification': `# Initialize Logistic Regression with optimized parameters
model = LogisticRegression(
    C=1.0,
    penalty='l2',
    solver='liblinear',
    random_state=42,
    max_iter=1000
)`,
        'regression': `# Logistic Regression not applicable for regression tasks
# Using Linear Regression instead
model = LinearRegression()`
      },
      'LinearRegression': {
        'regression': `# Initialize Linear Regression with regularization options
model = Ridge(alpha=1.0, random_state=42)`,
        'classification': `# Linear Regression not applicable for classification
# Using Logistic Regression instead
model = LogisticRegression(random_state=42)`
      },
      'LightGBM': {
        'classification': `# Initialize LightGBM Classifier with optimized parameters
model = LGBMClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)`,
        'regression': `# Initialize LightGBM Regressor with optimized parameters
model = LGBMRegressor(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)`
      },
      'CatBoost': {
        'classification': `# Initialize CatBoost Classifier with optimized parameters
model = CatBoostClassifier(
    iterations=300,
    depth=6,
    learning_rate=0.1,
    random_seed=42,
    verbose=False
)`,
        'regression': `# Initialize CatBoost Regressor with optimized parameters
model = CatBoostRegressor(
    iterations=300,
    depth=6,
    learning_rate=0.1,
    random_seed=42,
    verbose=False
)`
      },
      'SVM': {
        'classification': `# Initialize SVM Classifier with optimized parameters
model = SVC(
    C=1.0,
    kernel='rbf',
    gamma='scale',
    random_state=42
)`,
        'regression': `# Initialize SVR with optimized parameters
model = SVR(
    C=1.0,
    kernel='rbf',
    gamma='scale'
)`
      },
      'NaiveBayes': {
        'classification': `# Initialize Naive Bayes Classifier
model = GaussianNB()`,
        'regression': `# Naive Bayes not applicable for regression
# Using Linear Regression instead
model = LinearRegression()`
      }
    };

    const modelType = options.competitionType === 'regression' ? 'regression' : 'classification';
    const config = modelConfigs[options.selectedModel];
    
    return config?.[modelType] || config?.['classification'] || modelConfigs['XGBoost'][modelType];
  }

  private static getHyperparameterTuning(options: NotebookOptions): string {
    const tuningConfigs = {
      'XGBoost': `
# Hyperparameter tuning for XGBoost
print("üîç Performing hyperparameter tuning...")
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

random_search = RandomizedSearchCV(
    model, param_dist, n_iter=20, cv=3, 
    scoring='neg_mean_squared_error' if '${options.competitionType}' == 'regression' else 'accuracy',
    random_state=42, n_jobs=-1
)
random_search.fit(X_tr, y_tr)
model = random_search.best_estimator_
print(f"Best parameters: {random_search.best_params_}")`,
      
      'RandomForest': `
# Hyperparameter tuning for Random Forest
print("üîç Performing hyperparameter tuning...")
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

random_search = RandomizedSearchCV(
    model, param_dist, n_iter=20, cv=3,
    scoring='neg_mean_squared_error' if '${options.competitionType}' == 'regression' else 'accuracy',
    random_state=42, n_jobs=-1
)
random_search.fit(X_tr, y_tr)
model = random_search.best_estimator_
print(f"Best parameters: {random_search.best_params_}")`,
      
      'LogisticRegression': `
# Hyperparameter tuning for Logistic Regression
print("üîç Performing hyperparameter tuning...")
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1.0, 10.0],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

grid_search = GridSearchCV(
    model, param_grid, cv=3, scoring='accuracy', n_jobs=-1
)
grid_search.fit(X_tr, y_tr)
model = grid_search.best_estimator_
print(f"Best parameters: {grid_search.best_params_}")`
    };

    return tuningConfigs[options.selectedModel] || tuningConfigs['XGBoost'];
  }

  private static getAdvancedEvaluationCode(options: NotebookOptions): string {
    if (options.competitionType === 'regression') {
      return `# Advanced regression evaluation metrics
train_rmse = np.sqrt(mean_squared_error(y_tr, train_pred))
val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))
val_mae = mean_absolute_error(y_val, val_pred)
val_r2 = r2_score(y_val, val_pred)

# Additional metrics
val_mape = np.mean(np.abs((y_val - val_pred) / y_val)) * 100  # Mean Absolute Percentage Error
val_medae = np.median(np.abs(y_val - val_pred))  # Median Absolute Error

print(f"üìä Advanced Model Performance:")
print(f"Training RMSE: {train_rmse:.4f}")
print(f"Validation RMSE: {val_rmse:.4f}")
print(f"Validation MAE: {val_mae:.4f}")
print(f"Validation R¬≤: {val_r2:.4f}")
print(f"Validation MAPE: {val_mape:.2f}%")
print(f"Validation MedAE: {val_medae:.4f}")

# Advanced residual analysis
plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
plt.scatter(val_pred, y_val, alpha=0.6)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Predicted vs Actual')

plt.subplot(2, 3, 2)
residuals = y_val - val_pred
plt.scatter(val_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted')
plt.ylabel('Residuals')
plt.title('Residual Plot')

plt.subplot(2, 3, 3)
plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Distribution')

plt.subplot(2, 3, 4)
from scipy import stats
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot of Residuals')

plt.subplot(2, 3, 5)
plt.scatter(range(len(residuals)), residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Index')
plt.ylabel('Residuals')
plt.title('Residuals vs Index')

plt.subplot(2, 3, 6)
plt.boxplot(residuals)
plt.ylabel('Residuals')
plt.title('Residual Box Plot')

plt.tight_layout()
plt.show()`;
    } else {
      return `# Advanced classification evaluation metrics
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve

train_acc = accuracy_score(y_tr, train_pred)
val_acc = accuracy_score(y_val, val_pred)

# Additional metrics
val_precision = precision_score(y_val, val_pred, average='weighted')
val_recall = recall_score(y_val, val_pred, average='weighted')
val_f1 = f1_score(y_val, val_pred, average='weighted')

print(f"üìä Advanced Model Performance:")
print(f"Training Accuracy: {train_acc:.4f}")
print(f"Validation Accuracy: {val_acc:.4f}")
print(f"Validation Precision: {val_precision:.4f}")
print(f"Validation Recall: {val_recall:.4f}")
print(f"Validation F1-Score: {val_f1:.4f}")

# ROC AUC for binary classification
if len(np.unique(y)) == 2:
    if hasattr(model, 'predict_proba'):
        val_proba = model.predict_proba(X_val)[:, 1]
        val_auc = roc_auc_score(y_val, val_proba)
        print(f"Validation AUC: {val_auc:.4f}")

print(f"\\nüìã Detailed Classification Report:")
print(classification_report(y_val, val_pred))

# Advanced visualization
plt.figure(figsize=(15, 10))

# Confusion Matrix
plt.subplot(2, 3, 1)
cm = confusion_matrix(y_val, val_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')

# ROC Curve (for binary classification)
if len(np.unique(y)) == 2 and hasattr(model, 'predict_proba'):
    plt.subplot(2, 3, 2)
    fpr, tpr, _ = roc_curve(y_val, val_proba)
    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {val_auc:.3f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()

    # Precision-Recall Curve
    plt.subplot(2, 3, 3)
    precision, recall, _ = precision_recall_curve(y_val, val_proba)
    plt.plot(recall, precision)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')

# Class distribution
plt.subplot(2, 3, 4)
unique, counts = np.unique(y_val, return_counts=True)
plt.bar(unique, counts, alpha=0.7)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('True Class Distribution')

plt.subplot(2, 3, 5)
unique_pred, counts_pred = np.unique(val_pred, return_counts=True)
plt.bar(unique_pred, counts_pred, alpha=0.7)
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Predicted Class Distribution')

plt.tight_layout()
plt.show()`;
    }
  }

  private static getFeatureImportanceCode(selectedModel: string): string {
    const importanceCode = {
      'XGBoost': `
if hasattr(model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\\nüîù Top 20 Feature Importances (XGBoost):")
    print(feature_importance.head(20))
    
    # Plot feature importance
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(20)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Feature Importance')
    plt.title('Top 20 Feature Importances (XGBoost)')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()`,
      
      'RandomForest': `
if hasattr(model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\\nüîù Top 20 Feature Importances (Random Forest):")
    print(feature_importance.head(20))
    
    # Plot feature importance
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(20)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Feature Importance')
    plt.title('Top 20 Feature Importances (Random Forest)')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()`,
      
      'LogisticRegression': `
if hasattr(model, 'coef_'):
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'coefficient': model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_,
        'abs_coefficient': np.abs(model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_)
    }).sort_values('abs_coefficient', ascending=False)
    
    print("\\nüîù Top 20 Feature Coefficients (Logistic Regression):")
    print(feature_importance.head(20))
    
    # Plot feature coefficients
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(20)
    colors = ['red' if x < 0 else 'blue' for x in top_features['coefficient']]
    plt.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7)
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Coefficient Value')
    plt.title('Top 20 Feature Coefficients (Logistic Regression)')
    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()`
    };

    return importanceCode[selectedModel] || importanceCode['XGBoost'];
  }

  private static getModelSpecificTips(selectedModel: string): string {
    const tips = {
      'XGBoost': `- **Learning Rate Scheduling**: Try different learning rates with early stopping
- **Feature Interaction**: XGBoost automatically handles feature interactions
- **Regularization**: Tune alpha (L1) and lambda (L2) parameters
- **Tree Pruning**: Experiment with gamma parameter for tree pruning`,
      
      'RandomForest': `- **Bootstrap Sampling**: Tune bootstrap parameter for variance control
- **Feature Subsampling**: Optimize max_features for better generalization
- **Tree Depth**: Balance max_depth vs min_samples_leaf for overfitting control
- **Out-of-Bag Score**: Use oob_score=True for validation without separate set`,
      
      'LogisticRegression': `- **Regularization**: Try different C values and penalty types (L1, L2, ElasticNet)
- **Feature Scaling**: Ensure features are properly scaled for optimal performance
- **Solver Selection**: Choose appropriate solver based on dataset size and penalty
- **Class Balancing**: Use class_weight='balanced' for imbalanced datasets`,
      
      'LightGBM': `- **Categorical Features**: Use categorical_feature parameter for better handling
- **Early Stopping**: Implement early stopping with validation set
- **Leaf-wise Growth**: Tune num_leaves parameter carefully
- **Feature Fraction**: Use feature_fraction for feature subsampling`,
      
      'CatBoost': `- **Categorical Encoding**: CatBoost handles categorical features automatically
- **Overfitting Detection**: Use built-in overfitting detection
- **GPU Acceleration**: Enable GPU training for large datasets
- **Cross-validation**: Use built-in CV for robust model selection`
    };

    return tips[selectedModel] || tips['XGBoost'];
  }

  private static getSimplePredictions(type: NotebookOptions['competitionType']): string {
    if (type === 'regression') {
      return `# Simple baseline: use mean of target
mean_target = y.mean()
test_predictions = np.full(len(X_test), mean_target)
print(f"Using mean baseline: {mean_target:.4f}")`;
    } else {
      return `# Simple baseline: use mode of target  
mode_target = y.mode()[0]
test_predictions = np.full(len(X_test), mode_target)
print(f"Using mode baseline: {mode_target}")`;
    }
  }

  private static getAdvancedTips(type: NotebookOptions['competitionType']): string {
    const tips = {
      'classification': `- **Imbalanced Data**: Use SMOTE, ADASYN, or cost-sensitive learning
- **Feature Selection**: Try Recursive Feature Elimination (RFE) or LASSO
- **Threshold Tuning**: Optimize classification threshold for F1-score
- **Calibration**: Use Platt scaling or isotonic regression for probability calibration`,
      'regression': `- **Target Engineering**: Try log, sqrt, or Box-Cox transformations
- **Outlier Detection**: Use Isolation Forest or Local Outlier Factor
- **Residual Analysis**: Analyze residuals for model improvement insights
- **Quantile Regression**: Consider quantile regression for robust predictions`,
      'nlp': `- **Text Preprocessing**: Advanced tokenization, stemming, lemmatization
- **Feature Engineering**: N-grams, TF-IDF variations, word embeddings
- **Deep Learning**: BERT, RoBERTa, or domain-specific transformers
- **Data Augmentation**: Back-translation, synonym replacement`,
      'computer-vision': `- **Data Augmentation**: Advanced augmentation techniques
- **Transfer Learning**: Fine-tune pre-trained models (ResNet, EfficientNet)
- **Test Time Augmentation**: Average predictions over augmented test images
- **Model Architecture**: Experiment with different CNN architectures`,
      'other': `- **Domain Knowledge**: Incorporate domain-specific insights
- **External Data**: Consider additional relevant datasets
- **Feature Engineering**: Create domain-specific features
- **Validation Strategy**: Use appropriate CV for your data structure`
    };

    return tips[type];
  }
}